{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niv0902/Shablool/blob/main/Tut8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": [],\n",
        "      \"private_outputs\": true\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    },\n",
        "    \"accelerator\": \"GPU\"\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"# Generates images from text prompts.\\n\",\n",
        "        \"\\n\",\n",
        "        \"By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 602M parameter diffusion model trained on Conceptual 12M. See the GitHub repo for more information: https://github.com/crowsonkb/v-diffusion-pytorch.\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"sThFl4fJtEQJ\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# @title Licensed under the MIT License\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Copyright (c) 2022 Katherine Crowson\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Permission is hereby granted, free of charge, to any person obtaining a copy\\n\",\n",
        "        \"# of this software and associated documentation files (the \\\"Software\\\"), to deal\\n\",\n",
        "        \"# in the Software without restriction, including without limitation the rights\\n\",\n",
        "        \"# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n\",\n",
        "        \"# copies of the Software, and to permit persons to whom the Software is\\n\",\n",
        "        \"# furnished to do so, subject to the following conditions:\\n\",\n",
        "        \"\\n\",\n",
        "        \"# The above copyright notice and this permission notice shall be included in\\n\",\n",
        "        \"# all copies or substantial portions of the Software.\\n\",\n",
        "        \"\\n\",\n",
        "        \"# THE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n\",\n",
        "        \"# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n\",\n",
        "        \"# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n\",\n",
        "        \"# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n\",\n",
        "        \"# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n\",\n",
        "        \"# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\n\",\n",
        "        \"# THE SOFTWARE.\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"cellView\": \"form\",\n",
        "        \"id\": \"eb9FKu1rtH6R\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\": null,\n",
        "      \"metadata\": {\n",
        "        \"id\": \"xa4Er6nMgI1b\"\n",
        "      },\n",
        "      \"outputs\": [],\n",
        "      \"source\": [\n",
        "        \"# Check the GPU\\n\",\n",
        "        \"\\n\",\n",
        "        \"!nvidia-smi\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"### Install dependencies (no need to rerun this section if you restart the notebook runtime)\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"0XknUUiEsPCL\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# Install dependencies\\n\",\n",
        "        \"\\n\",\n",
        "        \"!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch\\n\",\n",
        "        \"%pip install git+https://github.com/crowsonkb/k-diffusion\\n\",\n",
        "        \"%pip install numpy==1.23.5\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"xQVAY5-4g8ta\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"If the \\\"Restart runtime\\\" button appears above, please click it to restart the runtime, then continue below.\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"Q7gmndIVKGbQ\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# Download the diffusion model\\n\",\n",
        "        \"# SHA-256: 4fc95ee1b3205a3f7422a07746383776e1dbc367eaf06a5b658ad351e77b7bda\\n\",\n",
        "        \"\\n\",\n",
        "        \"!mkdir v-diffusion-pytorch/checkpoints\\n\",\n",
        "        \"!curl -L --http1.1 https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1_cfg.pth > v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"9410kZ2aipVg\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"### Import modules and load models\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"tbjtOS68sazS\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# Imports\\n\",\n",
        "        \"\\n\",\n",
        "        \"import gc\\n\",\n",
        "        \"import math\\n\",\n",
        "        \"import sys\\n\",\n",
        "        \"\\n\",\n",
        "        \"import clip\\n\",\n",
        "        \"from IPython import display\\n\",\n",
        "        \"import k_diffusion as K\\n\",\n",
        "        \"import torch\\n\",\n",
        "        \"from torch import nn\\n\",\n",
        "        \"from torchvision import utils\\n\",\n",
        "        \"from torchvision.transforms import functional as TF\\n\",\n",
        "        \"from tqdm.notebook import trange, tqdm\\n\",\n",
        "        \"\\n\",\n",
        "        \"sys.path.append('/content/v-diffusion-pytorch')\\n\",\n",
        "        \"\\n\",\n",
        "        \"from diffusion import get_model\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"f_6OhF85i5vp\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# Load the models\\n\",\n",
        "        \"!mkdir -p v-diffusion-pytorch/checkpoints\\n\",\n",
        "        \"!wget https://huggingface.co/multimodalart/crowsonkb-v-diffusion-cc12m-1-cfg/resolve/main/cc12m_1_cfg.pth -O v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth\\n\",\n",
        "        \"\\n\",\n",
        "        \"inner_model = get_model('cc12m_1_cfg')()\\n\",\n",
        "        \"_, side_y, side_x = inner_model.shape\\n\",\n",
        "        \"inner_model.load_state_dict(torch.load('v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth', map_location='cpu',weights_only=False))\\n\",\n",
        "        \"inner_model = inner_model.half().cuda().eval().requires_grad_(False)\\n\",\n",
        "        \"model = K.external.VDenoiser(inner_model)\\n\",\n",
        "        \"clip_model = clip.load(inner_model.clip_model, jit=False, device='cpu')[0]\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"v7Zl0mg1jrBP\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"#@title Settings\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown The text prompt\\n\",\n",
        "        \"prompt = 'New York City, oil on canvas'  #@param {type:\\\"string\\\"}\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown The strength of the text conditioning (0 means don't condition on text, 1 means sample images that match the text about as well as the images match the text captions in the training set, 3+ is recommended).\\n\",\n",
        "        \"weight = 5  #@param {type:\\\"number\\\"}\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown Sample this many images.\\n\",\n",
        "        \"n_images = 4  #@param {type:\\\"integer\\\"}\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown Specify the number of diffusion timesteps (default is 50, can lower for faster but lower quality sampling).\\n\",\n",
        "        \"steps =   50#@param {type:\\\"integer\\\"}\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown The random seed. Change this to sample different images.\\n\",\n",
        "        \"seed = 0  #@param {type:\\\"integer\\\"}\\n\",\n",
        "        \"\\n\",\n",
        "        \"#@markdown Display progress every this many timesteps.\\n\",\n",
        "        \"display_every =   10#@param {type:\\\"integer\\\"}\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"cellView\": \"form\",\n",
        "        \"id\": \"kitsmKsekOJ_\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"### Actually do the run...\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"kuTFyUdbrlcK\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"target_embed = clip_model.encode_text(clip.tokenize(prompt)).float().cuda()\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"class CFGDenoiser(nn.Module):\\n\",\n",
        "        \"    def __init__(self, model, cond_scale):\\n\",\n",
        "        \"        super().__init__()\\n\",\n",
        "        \"        self.inner_model = model\\n\",\n",
        "        \"        self.cond_scale = cond_scale\\n\",\n",
        "        \"\\n\",\n",
        "        \"    def forward(self, x, sigma, clip_embed):\\n\",\n",
        "        \"        x_in = torch.cat([x] * 2)\\n\",\n",
        "        \"        sigma_in = torch.cat([sigma] * 2)\\n\",\n",
        "        \"        clip_embed_in = torch.cat([torch.zeros_like(clip_embed), clip_embed])\\n\",\n",
        "        \"        uncond, cond = self.inner_model(x_in, sigma_in, clip_embed=clip_embed_in).chunk(2)\\n\",\n",
        "        \"        return uncond + (cond - uncond) * self.cond_scale\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def callback(info):\\n\",\n",
        "        \"    if info['i'] % display_every == 0:\\n\",\n",
        "        \"        nrow = math.ceil(info['denoised'].shape[0] ** 0.5)\\n\",\n",
        "        \"        grid = utils.make_grid(info['denoised'], nrow, padding=0)\\n\",\n",
        "        \"        tqdm.write(f'Step {info[\\\"i\\\"]} of {steps}, sigma {info[\\\"sigma\\\"]:g}:')\\n\",\n",
        "        \"        display.display(K.utils.to_pil_image(grid))\\n\",\n",
        "        \"        tqdm.write(f'')\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"def run():\\n\",\n",
        "        \"    gc.collect()\\n\",\n",
        "        \"    torch.cuda.empty_cache()\\n\",\n",
        "        \"    torch.manual_seed(seed)\\n\",\n",
        "        \"    sigmas = K.sampling.get_sigmas_karras(steps, 1e-2, 160, device='cuda')\\n\",\n",
        "        \"    x = torch.randn([n_images, 3, side_y, side_x], device='cuda') * sigmas[0]\\n\",\n",
        "        \"    model_wrap = CFGDenoiser(torch.cuda.amp.autocast()(model), weight)\\n\",\n",
        "        \"    extra_args = {'clip_embed': target_embed.repeat([n_images, 1])}\\n\",\n",
        "        \"    outs = K.sampling.sample_lms(model_wrap, x, sigmas, extra_args=extra_args, callback=callback)\\n\",\n",
        "        \"    tqdm.write('Done!')\\n\",\n",
        "        \"    for i, out in enumerate(outs):\\n\",\n",
        "        \"        filename = f'out_{i}.png'\\n\",\n",
        "        \"        K.utils.to_pil_image(out).save(filename)\\n\",\n",
        "        \"        display.display(display.Image(filename))\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"run()\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"MnjPM_PUkYui\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"JP7fqew6jwK5\"\n",
        "      },\n",
        "      \"execution_count\": null,\n",
        "      \"outputs\": []\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "S_vFkGESKXwV",
        "outputId": "de627b1f-0ae2-4f4c-9c4e-e33775da0c90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'null' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3362717921>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     {\n\u001b[1;32m     18\u001b[0m       \u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"code\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0;34m\"execution_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \"metadata\": {\n\u001b[1;32m     21\u001b[0m         \"colab\": {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'null' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
